{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üè• Rural Emergency Triage AI - MedGemma Fine-Tuning Pipeline\n",
    "\n",
    "**MedGemma Impact Challenge Submission**\n",
    "\n",
    "This notebook fine-tunes **MedGemma 4B** for emergency radiology triage using the official Google approach:\n",
    "- **QLoRA** (4-bit quantization + LoRA adapters)\n",
    "- **SFTTrainer** from HuggingFace TRL\n",
    "- **Conversational format** (image + text prompt ‚Üí classification answer)\n",
    "\n",
    "### üéØ Training Configurations\n",
    "\n",
    "| Config | Samples | Epochs | Time (T4) | Time (A100) | Accuracy |\n",
    "|--------|---------|--------|-----------|-------------|----------|\n",
    "| **Quick Test** | 500 | 1 | 30 min | 10 min | 40-60% |\n",
    "| **Baseline** | 2,000 | 2 | 3 hours | 1 hour | 60-75% |\n",
    "| **Production** | 50,000 | 3 | 18 hours | 4 hours | 80-90% |\n",
    "| **Competition** | 100,000 | 5 | 48 hours | 12 hours | 85-95% |\n",
    "\n",
    "### üìã Requirements\n",
    "- **GPU**: A100 (40GB) via Colab Pro, or T4 (free tier)\n",
    "- **HuggingFace token**: With access to `google/medgemma-4b-it`\n",
    "- **Kaggle API key**: For dataset downloads\n",
    "\n",
    "### üöÄ Quick Start\n",
    "1. **Cell 8**: Choose dataset (small subset or full RSNA)\n",
    "2. **Cell 10**: Configure training scale (MAX_TRAIN, MAX_VAL)\n",
    "3. **Cell 14**: Set number of epochs (NUM_EPOCHS)\n",
    "4. **Run all cells** and wait for training to complete!\n",
    "\n",
    "### üìö Tasks Supported\n",
    "1. **Hemorrhage Detection** (CT head scans ‚Üí 6 subtypes)\n",
    "2. **Pneumothorax Detection** (Chest X-rays ‚Üí binary)\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works\n",
    "\n",
    "MedGemma is a **generative** vision-language model, not a traditional classifier.\n",
    "We frame classification as a multiple-choice question:\n",
    "\n",
    "```\n",
    "User: <image> What critical finding is present in this CT scan?\n",
    "A: No hemorrhage\n",
    "B: Epidural hemorrhage\n",
    "...\n",
    "Assistant: B: Epidural hemorrhage\n",
    "```\n",
    "\n",
    "The model learns to output the correct answer letter via supervised fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Tips for Large-Scale Training\n",
    "\n",
    "**For 50K+ samples:**\n",
    "- ‚úÖ Use Colab Pro with A100 GPU (4-8x faster than T4)\n",
    "- ‚úÖ Enable checkpointing in Cell 14 (save every 500 steps)\n",
    "- ‚úÖ Monitor with TensorBoard (see Cell 15)\n",
    "- ‚úÖ Keep browser tab active or use the JavaScript trick to prevent disconnection\n",
    "\n",
    "**Expected Results:**\n",
    "- 2K samples, 1 epoch: 60-75% accuracy\n",
    "- 50K samples, 3 epochs: 80-90% accuracy\n",
    "- 100K samples, 5 epochs: 85-95% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU - MedGemma requires bfloat16 support (A100 ideal, T4 works with adjustments)\n",
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    cap = torch.cuda.get_device_capability()\n",
    "    print(f\"Compute capability: {cap[0]}.{cap[1]}\")\n",
    "    print(f\"bfloat16 supported: {cap[0] >= 8}\")\n",
    "    if cap[0] < 8:\n",
    "        print(\"‚ö†Ô∏è GPU does not support bfloat16. Will use float16 instead.\")\n",
    "        print(\"   For best results, use an A100 GPU (Colab Pro).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q \"transformers>=4.50.0\" accelerate peft bitsandbytes\n",
    "!pip install -q trl datasets evaluate tensorboard\n",
    "!pip install -q pydicom opencv-python-headless Pillow\n",
    "!pip install -q scikit-learn pandas matplotlib seaborn\n",
    "!pip install -q kaggle tqdm pyyaml\n",
    "print(\"‚úì All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent model storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!mkdir -p /content/drive/MyDrive/rural_triage_ai/models\n",
    "!mkdir -p /content/drive/MyDrive/rural_triage_ai/results\n",
    "print(\"‚úì Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Step 2: Authentication\n",
    "\n",
    "You need two sets of credentials:\n",
    "1. **HuggingFace token** ‚Äî for MedGemma model access\n",
    "2. **Kaggle API key** ‚Äî for dataset downloads\n",
    "\n",
    "### Get MedGemma access:\n",
    "1. Go to [google/medgemma-4b-it](https://huggingface.co/google/medgemma-4b-it)\n",
    "2. Accept the usage conditions\n",
    "3. Get your token from [HF Settings](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# --- HuggingFace Authentication ---\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import userdata\n",
    "    try:\n",
    "        os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "        print(\"‚úì HF_TOKEN loaded from Colab Secrets\")\n",
    "    except Exception:\n",
    "        print(\"‚ö†Ô∏è HF_TOKEN not found in Colab Secrets.\")\n",
    "        print(\"   Add it: click üîë Secrets (left panel) ‚Üí New secret ‚Üí Name: HF_TOKEN\")\n",
    "        print(\"   Or run: huggingface-cli login\")\n",
    "        from huggingface_hub import notebook_login\n",
    "        notebook_login()\n",
    "else:\n",
    "    from huggingface_hub import get_token\n",
    "    if get_token() is None:\n",
    "        from huggingface_hub import notebook_login\n",
    "        notebook_login()\n",
    "\n",
    "# --- Kaggle Authentication ---\n",
    "if os.path.exists(\"/content/drive/MyDrive/kaggle.json\"):\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    print(\"‚úì Kaggle credentials loaded from Google Drive\")\n",
    "else:\n",
    "    print(\"Upload your kaggle.json (from https://www.kaggle.com/account ‚Üí Create New API Token):\")\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !mv kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    print(\"‚úì Kaggle credentials configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 3: Download Dataset\n",
    "\n",
    "We'll use the **RSNA Intracranial Hemorrhage Detection** dataset from Kaggle.\n",
    "For quick iteration, we download a small subset first.\n",
    "\n",
    "**Note**: You must accept the competition rules first at:\n",
    "https://www.kaggle.com/competitions/rsna-intracranial-hemorrhage-detection/rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = \"/content/data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üì• DATASET DOWNLOAD OPTIONS\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Choose your dataset based on the TASK in Cell 10:\")\n",
    "print()\n",
    "print(\"1Ô∏è‚É£  HEMORRHAGE DETECTION (CT scans)\")\n",
    "print(\"   Option A: Small subset (~500MB, 2000 images) - Quick testing\")\n",
    "print(\"   Option B: Full RSNA dataset (~100GB, 100K+ images) - Production\")\n",
    "print()\n",
    "print(\"2Ô∏è‚É£  PNEUMOTHORAX DETECTION (Chest X-rays)\")\n",
    "print(\"   SIIM-ACR dataset (~12GB)\")\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# OPTION A: Small Head CT Dataset (Quick Testing)\n",
    "# ============================================================\n",
    "print(\"Downloading small head CT hemorrhage dataset (~500MB)...\")\n",
    "print(\"Perfect for: Quick testing, pipeline validation\")\n",
    "print()\n",
    "os.makedirs(f\"{DATA_DIR}/head_ct\", exist_ok=True)\n",
    "!kaggle datasets download -d felipekitamura/head-ct-hemorrhage -p {DATA_DIR}/head_ct\n",
    "!cd {DATA_DIR}/head_ct && unzip -q -o \"*.zip\" 2>/dev/null; true\n",
    "print(\"‚úì Small dataset downloaded to /content/data/head_ct\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# OPTION B: Full RSNA Dataset (Production Training)\n",
    "# ============================================================\n",
    "# Uncomment these lines for full-scale training (100GB download)\n",
    "# \n",
    "# print(\"=\" * 60)\n",
    "# print(\"üì• DOWNLOADING FULL RSNA DATASET\")\n",
    "# print(\"=\" * 60)\n",
    "# print()\n",
    "# print(\"‚ö†Ô∏è  WARNING: This will download ~100GB of data\")\n",
    "# print(\"   Estimated time: 30-60 minutes\")\n",
    "# print(\"   Make sure you have accepted the competition rules at:\")\n",
    "# print(\"   https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/rules\")\n",
    "# print()\n",
    "# \n",
    "# os.makedirs(f\"{DATA_DIR}/rsna_hemorrhage\", exist_ok=True)\n",
    "# \n",
    "# # Download CSV labels\n",
    "# print(\"1/3 Downloading labels CSV...\")\n",
    "# !kaggle competitions download -c rsna-intracranial-hemorrhage-detection \\\n",
    "#   -f stage_2_train.csv -p {DATA_DIR}/rsna_hemorrhage\n",
    "# !cd {DATA_DIR}/rsna_hemorrhage && unzip -q stage_2_train.csv.zip\n",
    "# \n",
    "# # Download training images\n",
    "# print(\"2/3 Downloading training images (~100GB)...\")\n",
    "# !kaggle competitions download -c rsna-intracranial-hemorrhage-detection \\\n",
    "#   -f stage_2_train_images.zip -p {DATA_DIR}/rsna_hemorrhage\n",
    "# \n",
    "# # Extract images\n",
    "# print(\"3/3 Extracting images (this may take 20-30 minutes)...\")\n",
    "# !cd {DATA_DIR}/rsna_hemorrhage && unzip -q stage_2_train_images.zip\n",
    "# \n",
    "# # Verify\n",
    "# import glob\n",
    "# dcm_files = glob.glob(f\"{DATA_DIR}/rsna_hemorrhage/stage_2_train/*.dcm\")\n",
    "# print(f\"\\n‚úì Full RSNA dataset ready!\")\n",
    "# print(f\"  Total DICOM files: {len(dcm_files):,}\")\n",
    "# print(f\"  Location: {DATA_DIR}/rsna_hemorrhage\")\n",
    "\n",
    "# ============================================================\n",
    "# OPTION C: SIIM Pneumothorax Dataset (Chest X-rays)\n",
    "# ============================================================\n",
    "# Uncomment for pneumothorax training\n",
    "#\n",
    "# print(\"=\" * 60)\n",
    "# print(\"üì• DOWNLOADING SIIM-ACR PNEUMOTHORAX DATASET\")\n",
    "# print(\"=\" * 60)\n",
    "# print()\n",
    "# print(\"Size: ~12GB\")\n",
    "# print(\"Make sure you have accepted the competition rules at:\")\n",
    "# print(\"https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/rules\")\n",
    "# print()\n",
    "#\n",
    "# os.makedirs(f\"{DATA_DIR}/siim_pneumothorax\", exist_ok=True)\n",
    "#\n",
    "# # Download dataset\n",
    "# !kaggle competitions download -c siim-acr-pneumothorax-segmentation \\\n",
    "#   -p {DATA_DIR}/siim_pneumothorax\n",
    "#\n",
    "# # Extract\n",
    "# print(\"Extracting files...\")\n",
    "# !cd {DATA_DIR}/siim_pneumothorax && unzip -q \"*.zip\"\n",
    "#\n",
    "# print(f\"\\n‚úì SIIM dataset ready!\")\n",
    "# print(f\"  Location: {DATA_DIR}/siim_pneumothorax\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "!du -sh {DATA_DIR}/*\n",
    "print()\n",
    "print(\"‚úì Dataset download complete!\")\n",
    "print()\n",
    "print(\"üí° TIP: To use full RSNA dataset:\")\n",
    "print(\"   1. Uncomment 'OPTION B' section above\")\n",
    "print(\"   2. Set USE_FULL_RSNA = True in Cell 10\")\n",
    "print(\"   3. Re-run this cell and Cell 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Step 4: Prepare Dataset for MedGemma\n",
    "\n",
    "MedGemma uses a **conversational format** for fine-tuning. We convert our image classification\n",
    "dataset into message-based examples:\n",
    "\n",
    "```\n",
    "User: [image] What critical finding is present?\n",
    "Assistant: B: Epidural hemorrhage\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from PIL import Image\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# ============================================================\n",
    "# üéØ TRAINING CONFIGURATION - Adjust these for your needs\n",
    "# ============================================================\n",
    "\n",
    "# --- Task Selection ---\n",
    "TASK = \"hemorrhage\"  # Options: \"hemorrhage\" or \"pneumothorax\"\n",
    "\n",
    "# --- Training Scale ---\n",
    "# Quick test (30 min on T4):\n",
    "# MAX_TRAIN = 500\n",
    "# MAX_VAL = 50\n",
    "\n",
    "# Medium training (2-4 hours on T4):\n",
    "# MAX_TRAIN = 2000\n",
    "# MAX_VAL = 200\n",
    "\n",
    "# Full training (12-18 hours on T4, 3-4 hours on A100):\n",
    "MAX_TRAIN = 50000\n",
    "MAX_VAL = 5000\n",
    "\n",
    "# --- Dataset Source ---\n",
    "USE_FULL_RSNA = True  # Set to True to download full RSNA dataset (100GB)\n",
    "                       # Set to False to use small head CT subset (500MB)\n",
    "\n",
    "print(f\"üìä Configuration:\")\n",
    "print(f\"  Task: {TASK}\")\n",
    "print(f\"  Training samples: {MAX_TRAIN:,}\")\n",
    "print(f\"  Validation samples: {MAX_VAL:,}\")\n",
    "print(f\"  Full RSNA dataset: {USE_FULL_RSNA}\")\n",
    "\n",
    "# ============================================================\n",
    "# Task-specific settings\n",
    "# ============================================================\n",
    "if TASK == \"hemorrhage\":\n",
    "    CLASS_LABELS = [\n",
    "        \"A: No hemorrhage\",\n",
    "        \"B: Epidural hemorrhage\",\n",
    "        \"C: Subdural hemorrhage\",\n",
    "        \"D: Subarachnoid hemorrhage\",\n",
    "        \"E: Intraventricular hemorrhage\",\n",
    "        \"F: Intraparenchymal hemorrhage\",\n",
    "    ]\n",
    "    PROMPT = (\n",
    "        \"You are an emergency radiology AI assistant. \"\n",
    "        \"Analyze this CT head scan and identify the most likely finding.\\n\"\n",
    "        + \"\\n\".join(CLASS_LABELS)\n",
    "    )\n",
    "    DATA_DIR = \"/content/data/rsna_hemorrhage\" if USE_FULL_RSNA else \"/content/data/head_ct\"\n",
    "else:\n",
    "    CLASS_LABELS = [\n",
    "        \"A: No pneumothorax\",\n",
    "        \"B: Pneumothorax present\",\n",
    "    ]\n",
    "    PROMPT = (\n",
    "        \"You are an emergency radiology AI assistant. \"\n",
    "        \"Analyze this chest X-ray and determine if pneumothorax is present.\\n\"\n",
    "        + \"\\n\".join(CLASS_LABELS)\n",
    "    )\n",
    "    DATA_DIR = \"/content/data/siim_pneumothorax\"\n",
    "\n",
    "print(f\"\\n‚úì Task configured: {len(CLASS_LABELS)} classes\")\n",
    "\n",
    "# ============================================================\n",
    "# Build dataset from image files\n",
    "# ============================================================\n",
    "def load_rsna_full_dataset(data_dir: str) -> list[dict]:\n",
    "    \"\"\"Load full RSNA hemorrhage dataset with proper labels from CSV.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    csv_path = f\"{data_dir}/stage_2_train.csv\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"RSNA CSV not found at {csv_path}. Make sure dataset is downloaded.\")\n",
    "    \n",
    "    print(f\"Loading RSNA labels from {csv_path}...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Parse labels from CSV\n",
    "    # Format: ID_subtype where subtype is one of:\n",
    "    # any, epidural, intraparenchymal, intraventricular, subarachnoid, subdural\n",
    "    \n",
    "    # Group by image ID\n",
    "    image_labels = {}\n",
    "    for _, row in df.iterrows():\n",
    "        img_id = row['ID'].rsplit('_', 1)[0]\n",
    "        subtype = row['ID'].rsplit('_', 1)[1]\n",
    "        label_value = int(row['Label'])\n",
    "        \n",
    "        if img_id not in image_labels:\n",
    "            image_labels[img_id] = {}\n",
    "        image_labels[img_id][subtype] = label_value\n",
    "    \n",
    "    print(f\"Found {len(image_labels):,} unique images\")\n",
    "    \n",
    "    # Convert to examples with single dominant label\n",
    "    examples = []\n",
    "    for img_id, labels in image_labels.items():\n",
    "        img_path = f\"{data_dir}/stage_2_train/{img_id}.dcm\"\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        \n",
    "        # Determine dominant hemorrhage type (priority order)\n",
    "        if labels.get('epidural', 0) == 1:\n",
    "            label_idx = 1\n",
    "        elif labels.get('subdural', 0) == 1:\n",
    "            label_idx = 2\n",
    "        elif labels.get('subarachnoid', 0) == 1:\n",
    "            label_idx = 3\n",
    "        elif labels.get('intraventricular', 0) == 1:\n",
    "            label_idx = 4\n",
    "        elif labels.get('intraparenchymal', 0) == 1:\n",
    "            label_idx = 5\n",
    "        else:\n",
    "            label_idx = 0  # No hemorrhage\n",
    "        \n",
    "        examples.append({\n",
    "            \"image_path\": img_path,\n",
    "            \"label\": label_idx,\n",
    "        })\n",
    "    \n",
    "    print(f\"Loaded {len(examples):,} valid examples\")\n",
    "    return examples\n",
    "\n",
    "\n",
    "def find_images_and_labels(data_dir: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Scan data directory for images and assign labels.\n",
    "    Used for smaller datasets with folder-based labels.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    data_path = Path(data_dir)\n",
    "\n",
    "    # Strategy 1: Folder-based labels (e.g., chest_xray/train/NORMAL/, chest_xray/train/PNEUMONIA/)\n",
    "    for label_dir in sorted(data_path.rglob(\"*\")):\n",
    "        if label_dir.is_dir():\n",
    "            images = list(label_dir.glob(\"*.png\")) + list(label_dir.glob(\"*.jpg\")) + list(label_dir.glob(\"*.jpeg\"))\n",
    "            if images:\n",
    "                folder_name = label_dir.name.upper()\n",
    "                # Map folder names to label indices\n",
    "                if \"NORMAL\" in folder_name or \"NEGATIVE\" in folder_name or \"NO\" in folder_name:\n",
    "                    label_idx = 0\n",
    "                else:\n",
    "                    label_idx = min(1, len(CLASS_LABELS) - 1)\n",
    "                for img_path in images:\n",
    "                    examples.append({\"image_path\": str(img_path), \"label\": label_idx})\n",
    "\n",
    "    # Strategy 2: If no folder-based labels found, use all images\n",
    "    if not examples:\n",
    "        all_images = []\n",
    "        for ext in [\"*.png\", \"*.jpg\", \"*.jpeg\", \"*.dcm\"]:\n",
    "            all_images.extend(data_path.rglob(ext))\n",
    "        print(f\"Found {len(all_images)} images (no folder-based labels detected)\")\n",
    "        print(\"‚ö†Ô∏è Assigning random labels for demo. Replace with real labels!\")\n",
    "        for img_path in all_images:\n",
    "            examples.append({\n",
    "                \"image_path\": str(img_path),\n",
    "                \"label\": random.randint(0, len(CLASS_LABELS) - 1),\n",
    "            })\n",
    "\n",
    "    random.seed(42)\n",
    "    random.shuffle(examples)\n",
    "    return examples\n",
    "\n",
    "\n",
    "# Load dataset based on configuration\n",
    "print(\"\\nüì• Loading dataset...\")\n",
    "if TASK == \"hemorrhage\" and USE_FULL_RSNA:\n",
    "    raw_examples = load_rsna_full_dataset(DATA_DIR)\n",
    "else:\n",
    "    raw_examples = find_images_and_labels(DATA_DIR)\n",
    "\n",
    "print(f\"\\nTotal examples available: {len(raw_examples):,}\")\n",
    "\n",
    "# Limit dataset size based on configuration\n",
    "total_needed = MAX_TRAIN + MAX_VAL\n",
    "if len(raw_examples) < total_needed:\n",
    "    print(f\"‚ö†Ô∏è Warning: Only {len(raw_examples):,} examples available, requested {total_needed:,}\")\n",
    "    print(f\"   Using all available examples\")\n",
    "    raw_examples = raw_examples\n",
    "else:\n",
    "    raw_examples = raw_examples[:total_needed]\n",
    "\n",
    "# ============================================================\n",
    "# Convert to HuggingFace Dataset with conversational format\n",
    "# ============================================================\n",
    "def load_and_format(example: dict) -> dict:\n",
    "    \"\"\"Load image and format as MedGemma conversation.\"\"\"\n",
    "    try:\n",
    "        img_path = example[\"image_path\"]\n",
    "        if img_path.endswith(\".dcm\"):\n",
    "            import pydicom\n",
    "            dcm = pydicom.dcmread(img_path)\n",
    "            arr = dcm.pixel_array.astype(np.float32)\n",
    "            arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-8) * 255\n",
    "            image = Image.fromarray(arr.astype(np.uint8)).convert(\"RGB\")\n",
    "        else:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        label_idx = example[\"label\"]\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"label\": label_idx,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": PROMPT},\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": CLASS_LABELS[label_idx]},\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {example['image_path']}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process examples\n",
    "print(\"\\nüîÑ Loading and formatting images...\")\n",
    "formatted = []\n",
    "for i, ex in enumerate(raw_examples):\n",
    "    if i % 1000 == 0 and i > 0:\n",
    "        print(f\"  Processed {i:,}/{len(raw_examples):,} images...\")\n",
    "    result = load_and_format(ex)\n",
    "    if result is not None:\n",
    "        formatted.append(result)\n",
    "\n",
    "# Split into train/val\n",
    "split_idx = int(len(formatted) * 0.9)\n",
    "train_data = formatted[:split_idx]\n",
    "val_data = formatted[split_idx:]\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_data),\n",
    "    \"validation\": Dataset.from_list(val_data),\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úì Dataset ready!\")\n",
    "print(f\"  Train: {len(data['train']):,} examples\")\n",
    "print(f\"  Validation: {len(data['validation']):,} examples\")\n",
    "print(f\"\\nSample message format:\")\n",
    "print(data[\"train\"][0][\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 5: Load MedGemma with QLoRA\n",
    "\n",
    "We load MedGemma 4B instruction-tuned model with 4-bit quantization to fit in GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "MODEL_ID = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Determine dtype based on GPU capability\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
    "    compute_dtype = torch.bfloat16\n",
    "    print(\"Using bfloat16 (A100/H100 detected)\")\n",
    "else:\n",
    "    compute_dtype = torch.float16\n",
    "    print(\"Using float16 (T4/V100 detected)\")\n",
    "\n",
    "# QLoRA quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_quant_storage=compute_dtype,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_ID}...\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"\\n‚úì MedGemma loaded!\")\n",
    "print(f\"  Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"  Dtype: {compute_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from typing import Any\n",
    "\n",
    "# ============================================================\n",
    "# LoRA Configuration\n",
    "# ============================================================\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"],\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Custom Data Collator for multimodal inputs\n",
    "# ============================================================\n",
    "def collate_fn(examples: list[dict[str, Any]]):\n",
    "    \"\"\"Process examples with text + images into model input format.\"\"\"\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        images.append([example[\"image\"].convert(\"RGB\")])\n",
    "        texts.append(\n",
    "            processor.apply_chat_template(\n",
    "                example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "            ).strip()\n",
    "        )\n",
    "\n",
    "    # Tokenize texts and process images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Create labels: mask padding and image tokens in loss computation\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # Mask image tokens\n",
    "    image_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map.get(\"boi_token\", \"<image>\")\n",
    "    )\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    if isinstance(image_token_id, int):\n",
    "        labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100  # Additional image placeholder token\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "print(\"‚úì LoRA config and data collator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# ============================================================\n",
    "# üéØ TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "OUTPUT_DIR = f\"medgemma-4b-it-{TASK}\"\n",
    "DRIVE_SAVE_DIR = f\"/content/drive/MyDrive/rural_triage_ai/models/{TASK}\"\n",
    "os.makedirs(DRIVE_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- Training Duration ---\n",
    "# Quick test (30 min on T4):\n",
    "# NUM_EPOCHS = 1\n",
    "\n",
    "# Medium training (2-4 hours on T4):\n",
    "# NUM_EPOCHS = 2\n",
    "\n",
    "# Full training (12-18 hours on T4, 3-4 hours on A100):\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# --- Batch Size Configuration ---\n",
    "# Automatically adjusted based on GPU capability\n",
    "# A100 (40GB): batch_size=4, grad_accum=4 ‚Üí effective batch size = 16\n",
    "# T4 (16GB):   batch_size=1, grad_accum=16 ‚Üí effective batch size = 16\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
    "    batch_size = 4\n",
    "    grad_accum = 4\n",
    "    print(\"üöÄ A100 GPU detected - Using optimized batch size\")\n",
    "else:\n",
    "    batch_size = 1\n",
    "    grad_accum = 16\n",
    "    print(\"‚ö° T4 GPU detected - Using memory-efficient batch size\")\n",
    "\n",
    "# --- Learning Rate ---\n",
    "# Default works well for most cases\n",
    "# Increase for faster convergence (but risk instability)\n",
    "# Decrease for more stable training (but slower)\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# --- Evaluation Frequency ---\n",
    "# Evaluate every N steps to monitor progress\n",
    "EVAL_STEPS = 50 if len(data['train']) < 5000 else 200\n",
    "\n",
    "# --- Checkpointing ---\n",
    "# Save model every epoch (recommended for long training)\n",
    "SAVE_STRATEGY = \"epoch\"\n",
    "# Uncomment to save every N steps instead:\n",
    "# SAVE_STRATEGY = \"steps\"\n",
    "# SAVE_STEPS = 500\n",
    "\n",
    "print(f\"\\nüìä Training Configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {batch_size} √ó {grad_accum} grad accum = {batch_size * grad_accum} effective\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Eval steps: {EVAL_STEPS}\")\n",
    "print(f\"  Training steps per epoch: ~{len(data['train']) // (batch_size * grad_accum)}\")\n",
    "print(f\"  Total training steps: ~{(len(data['train']) // (batch_size * grad_accum)) * NUM_EPOCHS}\")\n",
    "print()\n",
    "\n",
    "# Estimate training time\n",
    "steps_per_epoch = len(data['train']) // (batch_size * grad_accum)\n",
    "total_steps = steps_per_epoch * NUM_EPOCHS\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
    "    # A100: ~2-3 seconds per step\n",
    "    estimated_hours = (total_steps * 2.5) / 3600\n",
    "    gpu_name = \"A100\"\n",
    "else:\n",
    "    # T4: ~8-10 seconds per step\n",
    "    estimated_hours = (total_steps * 9) / 3600\n",
    "    gpu_name = \"T4\"\n",
    "\n",
    "print(f\"‚è±Ô∏è  Estimated training time on {gpu_name}: {estimated_hours:.1f} hours\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# Training Arguments\n",
    "# ============================================================\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    bf16=(compute_dtype == torch.bfloat16),\n",
    "    fp16=(compute_dtype == torch.float16),\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Create Trainer\n",
    "# ============================================================\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Trainer configured!\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print(f\"  Drive backup: {DRIVE_SAVE_DIR}\")\n",
    "print()\n",
    "print(\"üí° TIP: Monitor training in real-time with TensorBoard:\")\n",
    "print(\"   Run this in a new cell: %tensorboard --logdir {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 6a: Monitor Training (Optional)\n",
    "\n",
    "Run this cell **while training is in progress** to see real-time metrics in TensorBoard.\n",
    "\n",
    "This shows:\n",
    "- Training loss over time\n",
    "- Validation loss\n",
    "- Learning rate schedule\n",
    "- GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Launch TensorBoard\n",
    "%tensorboard --logdir {OUTPUT_DIR}\n",
    "\n",
    "# You can also view logs manually:\n",
    "# !ls -lh {OUTPUT_DIR}/runs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ude80 Step 6: Train!\n",
    "\n",
    "This will take approximately:\n",
    "- **A100**: ~1-3 hours (2000 samples, 1 epoch)\n",
    "- **T4**: ~4-8 hours (2000 samples, 1 epoch)\n",
    "\n",
    "You can safely close your laptop ‚Äî training runs in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"   Task: {TASK}\")\n",
    "print(f\"   Model: {MODEL_ID}\")\n",
    "print(f\"   Train samples: {len(data['train'])}\")\n",
    "print(f\"   Val samples: {len(data['validation'])}\")\n",
    "print()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Save Model to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import shutil\n",
    "import json\n",
    "\n",
    "# Save LoRA adapter locally\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Copy to Google Drive for persistence\n",
    "print(f\"Copying model to Google Drive: {DRIVE_SAVE_DIR}\")\n",
    "if os.path.exists(DRIVE_SAVE_DIR):\n",
    "    shutil.rmtree(DRIVE_SAVE_DIR)\n",
    "shutil.copytree(OUTPUT_DIR, DRIVE_SAVE_DIR)\n",
    "\n",
    "# Save training metrics\n",
    "metrics = trainer.state.log_history\n",
    "with open(f\"{DRIVE_SAVE_DIR}/training_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# Save task config for inference\n",
    "task_config = {\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"task\": TASK,\n",
    "    \"class_labels\": CLASS_LABELS,\n",
    "    \"prompt\": PROMPT,\n",
    "    \"lora_dir\": DRIVE_SAVE_DIR,\n",
    "}\n",
    "with open(f\"{DRIVE_SAVE_DIR}/task_config.json\", \"w\") as f:\n",
    "    json.dump(task_config, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Model saved to Google Drive!\")\n",
    "print(f\"  Adapter: {DRIVE_SAVE_DIR}/\")\n",
    "print(f\"  Metrics: {DRIVE_SAVE_DIR}/training_metrics.json\")\n",
    "print(f\"  Config:  {DRIVE_SAVE_DIR}/task_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================\n",
    "# Run inference on validation set\n",
    "# ============================================================\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_pred_text = []\n",
    "num_eval = min(100, len(data[\"validation\"]))  # Evaluate on subset for speed\n",
    "\n",
    "print(f\"Running inference on {num_eval} validation samples...\\n\")\n",
    "\n",
    "for i in range(num_eval):\n",
    "    example = data[\"validation\"][i]\n",
    "    image = example[\"image\"].convert(\"RGB\")\n",
    "    true_label = example[\"label\"]\n",
    "\n",
    "    # Build input (user message only, no assistant response)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": PROMPT},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, tokenize=False\n",
    "    )\n",
    "    model_inputs = processor(\n",
    "        text=inputs, images=[image], return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=compute_dtype)\n",
    "\n",
    "    input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "    \n",
    "    generated = processor.decode(output[0][input_len:], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Parse predicted class\n",
    "    pred_label = -1\n",
    "    for idx, class_name in enumerate(CLASS_LABELS):\n",
    "        if class_name.split(\":\")[0].strip() in generated:\n",
    "            pred_label = idx\n",
    "            break\n",
    "    if pred_label == -1:\n",
    "        pred_label = 0  # Default fallback\n",
    "\n",
    "    y_true.append(true_label)\n",
    "    y_pred.append(pred_label)\n",
    "    y_pred_text.append(generated)\n",
    "\n",
    "    if i < 5:\n",
    "        print(f\"  Sample {i}: True={CLASS_LABELS[true_label]} | Pred={generated}\")\n",
    "\n",
    "# ============================================================\n",
    "# Classification Report\n",
    "# ============================================================\n",
    "short_labels = [c.split(\": \")[1] for c in CLASS_LABELS]\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Classification Report\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_true, y_pred, target_names=short_labels, zero_division=0))\n",
    "\n",
    "# ============================================================\n",
    "# Confusion Matrix\n",
    "# ============================================================\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(CLASS_LABELS))))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "    xticklabels=short_labels, yticklabels=short_labels,\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(f\"Confusion Matrix ‚Äî {TASK.title()} Detection\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{DRIVE_SAVE_DIR}/confusion_matrix.png\", dpi=150)\n",
    "plt.show()\n",
    "print(f\"‚úì Confusion matrix saved to {DRIVE_SAVE_DIR}/confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ TRAINING PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "What we accomplished:\n",
    "  ‚úì Loaded MedGemma 4B with QLoRA (4-bit quantization)\n",
    "  ‚úì Prepared {len(data['train']):,} training samples in conversational format\n",
    "  ‚úì Fine-tuned for {NUM_EPOCHS} epoch(s) with SFTTrainer + LoRA adapters\n",
    "  ‚úì Evaluated on {len(data['validation']):,} validation samples\n",
    "  ‚úì Saved LoRA adapter + config to Google Drive\n",
    "\n",
    "Model saved at:\n",
    "  \udcc1 Google Drive: {DRIVE_SAVE_DIR}/\n",
    "  üìÅ Colab (temporary): {OUTPUT_DIR}/\n",
    "\n",
    "Files saved:\n",
    "  ‚Ä¢ adapter_model.safetensors - LoRA weights\n",
    "  ‚Ä¢ adapter_config.json - LoRA configuration\n",
    "  ‚Ä¢ task_config.json - Task and class labels\n",
    "  ‚Ä¢ training_metrics.json - Training history\n",
    "  ‚Ä¢ confusion_matrix.png - Evaluation results\n",
    "\n",
    "To load the fine-tuned model later:\n",
    "  \n",
    "  from peft import PeftModel\n",
    "  from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "  \n",
    "  base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "      \"{MODEL_ID}\",\n",
    "      torch_dtype=torch.float16,\n",
    "      device_map=\"auto\"\n",
    "  )\n",
    "  model = PeftModel.from_pretrained(base_model, \"{DRIVE_SAVE_DIR}\")\n",
    "  processor = AutoProcessor.from_pretrained(\"{DRIVE_SAVE_DIR}\")\n",
    "\n",
    "Or use the inference script locally:\n",
    "  \n",
    "  from src.models.medgemma.inference import MedGemmaInference\n",
    "  \n",
    "  model = MedGemmaInference(lora_adapter_path=\"./data/models/{TASK}\")\n",
    "  result = model.predict(\"ct_scan.dcm\")\n",
    "  print(result['predicted_class'])\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ NEXT STEPS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "1Ô∏è‚É£  Scale Up Training (for better accuracy):\n",
    "   ‚Ä¢ Cell 10: Increase MAX_TRAIN to 50,000+\n",
    "   ‚Ä¢ Cell 14: Increase NUM_EPOCHS to 3-5\n",
    "   ‚Ä¢ Cell 8: Uncomment full RSNA dataset download\n",
    "   ‚Ä¢ Expected: 80-90% accuracy\n",
    "\n",
    "2Ô∏è‚É£  Train Different Task:\n",
    "   ‚Ä¢ Cell 10: Change TASK = \"pneumothorax\"\n",
    "   ‚Ä¢ Cell 8: Uncomment SIIM dataset download\n",
    "   ‚Ä¢ Re-run all cells\n",
    "\n",
    "3Ô∏è‚É£  Download Model to Local Machine:\n",
    "   ‚Ä¢ Go to Google Drive: rural_triage_ai/models/{TASK}/\n",
    "   ‚Ä¢ Download folder\n",
    "   ‚Ä¢ Extract to: ./data/models/{TASK}/\n",
    "   ‚Ä¢ Use with inference.py script\n",
    "\n",
    "4Ô∏è‚É£  Integrate into Application:\n",
    "   ‚Ä¢ Add to FastAPI backend (src/api/)\n",
    "   ‚Ä¢ Build React Native UI\n",
    "   ‚Ä¢ Deploy to tablet for offline use\n",
    "\n",
    "5Ô∏è‚É£  Submit to Kaggle Competition:\n",
    "   ‚Ä¢ Generate predictions on test set\n",
    "   ‚Ä¢ Format submission CSV\n",
    "   ‚Ä¢ Upload to MedGemma Impact Challenge\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìö DOCUMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "‚Ä¢ Quick Start: QUICK_START.md\n",
    "‚Ä¢ Scaling Guide: docs/SCALING_TRAINING.md\n",
    "‚Ä¢ Local Inference: docs/LOCAL_INFERENCE.md\n",
    "‚Ä¢ Colab Guide: docs/COLAB_QUICKSTART.md\n",
    "\n",
    "GitHub: https://github.com/ApoorvThite/rural-emergency-triage-ai\n",
    "\"\"\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"üéâ Happy training! Good luck with the MedGemma Impact Challenge!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path: str, task_prompt: str = PROMPT) -> str:\n",
    "    \"\"\"Run inference on a single image and return the prediction.\"\"\"\n",
    "    if image_path.endswith(\".dcm\"):\n",
    "        import pydicom\n",
    "        dcm = pydicom.dcmread(image_path)\n",
    "        arr = dcm.pixel_array.astype(np.float32)\n",
    "        arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-8) * 255\n",
    "        image = Image.fromarray(arr.astype(np.uint8)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": task_prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, tokenize=False\n",
    "    )\n",
    "    model_inputs = processor(\n",
    "        text=inputs, images=[image], return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=compute_dtype)\n",
    "\n",
    "    input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "\n",
    "    response = processor.decode(output[0][input_len:], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Display\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Prediction: {response}\", fontsize=12, pad=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# --- Test on a validation image ---\n",
    "test_example = data[\"validation\"][0]\n",
    "test_path = test_example.get(\"image_path\", None)\n",
    "if test_path and os.path.exists(test_path):\n",
    "    result = predict_image(test_path)\n",
    "else:\n",
    "    # Use the PIL image directly\n",
    "    image = test_example[\"image\"].convert(\"RGB\")\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": PROMPT}]}]\n",
    "    inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    model_inputs = processor(text=inputs, images=[image], return_tensors=\"pt\").to(model.device, dtype=compute_dtype)\n",
    "    input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "    result = processor.decode(output[0][input_len:], skip_special_tokens=True).strip()\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Prediction: {result}\", fontsize=12, pad=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nTrue label: {CLASS_LABELS[test_example['label']]}\")\n",
    "print(f\"Prediction: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "What we did:\n",
    "  ‚úì Loaded MedGemma 4B with QLoRA (4-bit quantization)\n",
    "  ‚úì Prepared {TASK} dataset in conversational format\n",
    "  ‚úì Fine-tuned with SFTTrainer + LoRA adapters\n",
    "  ‚úì Evaluated on validation set with classification metrics\n",
    "  ‚úì Saved LoRA adapter + config to Google Drive\n",
    "\n",
    "Model saved at:\n",
    "  {DRIVE_SAVE_DIR}/\n",
    "\n",
    "To load the fine-tuned model later:\n",
    "  from peft import PeftModel\n",
    "  base_model = AutoModelForImageTextToText.from_pretrained(\"{MODEL_ID}\", ...)\n",
    "  model = PeftModel.from_pretrained(base_model, \"{DRIVE_SAVE_DIR}\")\n",
    "\n",
    "Next steps:\n",
    "  1. Train on full RSNA dataset (100K+ images) for better accuracy\n",
    "  2. Train pneumothorax task (change TASK=\"pneumothorax\" and re-run)\n",
    "  3. Integrate into FastAPI backend (src/api/)\n",
    "  4. Build React Native UI for tablet deployment\n",
    "  5. Submit to MedGemma Impact Challenge!\n",
    "\"\"\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
