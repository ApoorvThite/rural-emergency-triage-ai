{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üè• Rural Emergency Triage AI - MedGemma Fine-Tuning Pipeline\n",
    "\n",
    "**MedGemma Impact Challenge Submission**\n",
    "\n",
    "This notebook fine-tunes **MedGemma 4B** for emergency radiology triage using the official Google approach:\n",
    "- **QLoRA** (4-bit quantization + LoRA adapters)\n",
    "- **SFTTrainer** from HuggingFace TRL\n",
    "- **Conversational format** (image + text prompt ‚Üí classification answer)\n",
    "\n",
    "### Requirements\n",
    "- **GPU**: A100 (40GB) via Colab Pro, or T4 with reduced batch size\n",
    "- **HuggingFace token**: With access to `google/medgemma-4b-it`\n",
    "- **Kaggle API key**: For dataset downloads\n",
    "\n",
    "### Tasks\n",
    "1. **Hemorrhage Detection** (CT head scans ‚Üí 6 subtypes)\n",
    "2. **Pneumothorax Detection** (Chest X-rays ‚Üí binary)\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works\n",
    "\n",
    "MedGemma is a **generative** vision-language model, not a traditional classifier.\n",
    "We frame classification as a multiple-choice question:\n",
    "\n",
    "```\n",
    "User: <image> What critical finding is present in this CT scan?\n",
    "A: No hemorrhage\n",
    "B: Epidural hemorrhage\n",
    "...\n",
    "Assistant: B: Epidural hemorrhage\n",
    "```\n",
    "\n",
    "The model learns to output the correct answer letter via supervised fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU - MedGemma requires bfloat16 support (A100 ideal, T4 works with adjustments)\n",
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    cap = torch.cuda.get_device_capability()\n",
    "    print(f\"Compute capability: {cap[0]}.{cap[1]}\")\n",
    "    print(f\"bfloat16 supported: {cap[0] >= 8}\")\n",
    "    if cap[0] < 8:\n",
    "        print(\"‚ö†Ô∏è GPU does not support bfloat16. Will use float16 instead.\")\n",
    "        print(\"   For best results, use an A100 GPU (Colab Pro).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q \"transformers>=4.50.0\" accelerate peft bitsandbytes\n",
    "!pip install -q trl datasets evaluate tensorboard\n",
    "!pip install -q pydicom opencv-python-headless Pillow\n",
    "!pip install -q scikit-learn pandas matplotlib seaborn\n",
    "!pip install -q kaggle tqdm pyyaml\n",
    "print(\"‚úì All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent model storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!mkdir -p /content/drive/MyDrive/rural_triage_ai/models\n",
    "!mkdir -p /content/drive/MyDrive/rural_triage_ai/results\n",
    "print(\"‚úì Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Step 2: Authentication\n",
    "\n",
    "You need two sets of credentials:\n",
    "1. **HuggingFace token** ‚Äî for MedGemma model access\n",
    "2. **Kaggle API key** ‚Äî for dataset downloads\n",
    "\n",
    "### Get MedGemma access:\n",
    "1. Go to [google/medgemma-4b-it](https://huggingface.co/google/medgemma-4b-it)\n",
    "2. Accept the usage conditions\n",
    "3. Get your token from [HF Settings](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# --- HuggingFace Authentication ---\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import userdata\n",
    "    try:\n",
    "        os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "        print(\"‚úì HF_TOKEN loaded from Colab Secrets\")\n",
    "    except Exception:\n",
    "        print(\"‚ö†Ô∏è HF_TOKEN not found in Colab Secrets.\")\n",
    "        print(\"   Add it: click üîë Secrets (left panel) ‚Üí New secret ‚Üí Name: HF_TOKEN\")\n",
    "        print(\"   Or run: huggingface-cli login\")\n",
    "        from huggingface_hub import notebook_login\n",
    "        notebook_login()\n",
    "else:\n",
    "    from huggingface_hub import get_token\n",
    "    if get_token() is None:\n",
    "        from huggingface_hub import notebook_login\n",
    "        notebook_login()\n",
    "\n",
    "# --- Kaggle Authentication ---\n",
    "if os.path.exists(\"/content/drive/MyDrive/kaggle.json\"):\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    print(\"‚úì Kaggle credentials loaded from Google Drive\")\n",
    "else:\n",
    "    print(\"Upload your kaggle.json (from https://www.kaggle.com/account ‚Üí Create New API Token):\")\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !mv kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    print(\"‚úì Kaggle credentials configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 3: Download Dataset\n",
    "\n",
    "We'll use the **RSNA Intracranial Hemorrhage Detection** dataset from Kaggle.\n",
    "For quick iteration, we download a small subset first.\n",
    "\n",
    "**Note**: You must accept the competition rules first at:\n",
    "https://www.kaggle.com/competitions/rsna-intracranial-hemorrhage-detection/rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = \"/content/data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# --- Option A: Download RSNA Hemorrhage subset from Kaggle ---\n",
    "# Uncomment if you have Kaggle credentials and accepted competition rules:\n",
    "# !kaggle competitions download -c rsna-intracranial-hemorrhage-detection -p {DATA_DIR}/rsna -f stage_2_train.csv\n",
    "# !kaggle competitions download -c rsna-intracranial-hemorrhage-detection -p {DATA_DIR}/rsna -f stage_2_train_images.zip\n",
    "\n",
    "# --- Option B: Download a smaller public head CT dataset ---\n",
    "print(\"Downloading head CT hemorrhage dataset (~500MB)...\")\n",
    "os.makedirs(f\"{DATA_DIR}/head_ct\", exist_ok=True)\n",
    "!kaggle datasets download -d felipekitamura/head-ct-hemorrhage -p {DATA_DIR}/head_ct\n",
    "!cd {DATA_DIR}/head_ct && unzip -q -o \"*.zip\" 2>/dev/null; true\n",
    "\n",
    "# --- Option C: Use chest X-ray dataset for pneumothorax ---\n",
    "# print(\"Downloading chest X-ray pneumonia dataset (~1.2GB)...\")\n",
    "# os.makedirs(f\"{DATA_DIR}/chest_xray\", exist_ok=True)\n",
    "# !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia -p {DATA_DIR}/chest_xray\n",
    "# !cd {DATA_DIR}/chest_xray && unzip -q -o \"*.zip\" 2>/dev/null; true\n",
    "\n",
    "print(\"\\n‚úì Dataset downloaded!\")\n",
    "!du -sh {DATA_DIR}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Step 4: Prepare Dataset for MedGemma\n",
    "\n",
    "MedGemma uses a **conversational format** for fine-tuning. We convert our image classification\n",
    "dataset into message-based examples:\n",
    "\n",
    "```\n",
    "User: [image] What critical finding is present?\n",
    "Assistant: B: Epidural hemorrhage\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from PIL import Image\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# ============================================================\n",
    "# Configuration ‚Äî change these for your task\n",
    "# ============================================================\n",
    "TASK = \"hemorrhage\"  # \"hemorrhage\" or \"pneumothorax\"\n",
    "\n",
    "if TASK == \"hemorrhage\":\n",
    "    CLASS_LABELS = [\n",
    "        \"A: No hemorrhage\",\n",
    "        \"B: Epidural hemorrhage\",\n",
    "        \"C: Subdural hemorrhage\",\n",
    "        \"D: Subarachnoid hemorrhage\",\n",
    "        \"E: Intraventricular hemorrhage\",\n",
    "        \"F: Intraparenchymal hemorrhage\",\n",
    "    ]\n",
    "    PROMPT = (\n",
    "        \"You are an emergency radiology AI assistant. \"\n",
    "        \"Analyze this CT head scan and identify the most likely finding.\\n\"\n",
    "        + \"\\n\".join(CLASS_LABELS)\n",
    "    )\n",
    "    DATA_DIR = \"/content/data/head_ct\"\n",
    "else:\n",
    "    CLASS_LABELS = [\n",
    "        \"A: No pneumothorax\",\n",
    "        \"B: Pneumothorax present\",\n",
    "    ]\n",
    "    PROMPT = (\n",
    "        \"You are an emergency radiology AI assistant. \"\n",
    "        \"Analyze this chest X-ray and determine if pneumothorax is present.\\n\"\n",
    "        + \"\\n\".join(CLASS_LABELS)\n",
    "    )\n",
    "    DATA_DIR = \"/content/data/chest_xray\"\n",
    "\n",
    "print(f\"Task: {TASK}\")\n",
    "print(f\"Classes: {len(CLASS_LABELS)}\")\n",
    "print(f\"Data dir: {DATA_DIR}\")\n",
    "\n",
    "# ============================================================\n",
    "# Build dataset from image files\n",
    "# ============================================================\n",
    "def find_images_and_labels(data_dir: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Scan data directory for images and assign labels.\n",
    "    Adapt this function to your specific dataset structure.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    data_path = Path(data_dir)\n",
    "\n",
    "    # Strategy 1: Folder-based labels (e.g., chest_xray/train/NORMAL/, chest_xray/train/PNEUMONIA/)\n",
    "    for label_dir in sorted(data_path.rglob(\"*\")):\n",
    "        if label_dir.is_dir():\n",
    "            images = list(label_dir.glob(\"*.png\")) + list(label_dir.glob(\"*.jpg\")) + list(label_dir.glob(\"*.jpeg\"))\n",
    "            if images:\n",
    "                folder_name = label_dir.name.upper()\n",
    "                # Map folder names to label indices\n",
    "                if \"NORMAL\" in folder_name or \"NEGATIVE\" in folder_name or \"NO\" in folder_name:\n",
    "                    label_idx = 0\n",
    "                else:\n",
    "                    label_idx = min(1, len(CLASS_LABELS) - 1)\n",
    "                for img_path in images:\n",
    "                    examples.append({\"image_path\": str(img_path), \"label\": label_idx})\n",
    "\n",
    "    # Strategy 2: If no folder-based labels found, use all images with label 0\n",
    "    # (You'll need to add proper labels from a CSV or other source)\n",
    "    if not examples:\n",
    "        all_images = []\n",
    "        for ext in [\"*.png\", \"*.jpg\", \"*.jpeg\", \"*.dcm\"]:\n",
    "            all_images.extend(data_path.rglob(ext))\n",
    "        print(f\"Found {len(all_images)} images (no folder-based labels detected)\")\n",
    "        print(\"‚ö†Ô∏è Assigning random labels for demo. Replace with real labels!\")\n",
    "        for img_path in all_images:\n",
    "            examples.append({\n",
    "                \"image_path\": str(img_path),\n",
    "                \"label\": random.randint(0, len(CLASS_LABELS) - 1),\n",
    "            })\n",
    "\n",
    "    random.seed(42)\n",
    "    random.shuffle(examples)\n",
    "    return examples\n",
    "\n",
    "raw_examples = find_images_and_labels(DATA_DIR)\n",
    "print(f\"\\nTotal examples: {len(raw_examples)}\")\n",
    "\n",
    "# Limit dataset size for quick iteration (increase for full training)\n",
    "MAX_TRAIN = 2000\n",
    "MAX_VAL = 200\n",
    "raw_examples = raw_examples[: MAX_TRAIN + MAX_VAL]\n",
    "\n",
    "# ============================================================\n",
    "# Convert to HuggingFace Dataset with conversational format\n",
    "# ============================================================\n",
    "def load_and_format(example: dict) -> dict:\n",
    "    \"\"\"Load image and format as MedGemma conversation.\"\"\"\n",
    "    try:\n",
    "        img_path = example[\"image_path\"]\n",
    "        if img_path.endswith(\".dcm\"):\n",
    "            import pydicom\n",
    "            dcm = pydicom.dcmread(img_path)\n",
    "            arr = dcm.pixel_array.astype(np.float32)\n",
    "            arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-8) * 255\n",
    "            image = Image.fromarray(arr.astype(np.uint8)).convert(\"RGB\")\n",
    "        else:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        label_idx = example[\"label\"]\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"label\": label_idx,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": PROMPT},\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": CLASS_LABELS[label_idx]},\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {example['image_path']}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process examples\n",
    "print(\"Loading and formatting images...\")\n",
    "formatted = []\n",
    "for ex in raw_examples:\n",
    "    result = load_and_format(ex)\n",
    "    if result is not None:\n",
    "        formatted.append(result)\n",
    "\n",
    "# Split into train/val\n",
    "split_idx = int(len(formatted) * 0.9)\n",
    "train_data = formatted[:split_idx]\n",
    "val_data = formatted[split_idx:]\n",
    "\n",
    "data = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_data),\n",
    "    \"validation\": Dataset.from_list(val_data),\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úì Dataset ready!\")\n",
    "print(f\"  Train: {len(data['train'])} examples\")\n",
    "print(f\"  Validation: {len(data['validation'])} examples\")\n",
    "print(f\"\\nSample message format:\")\n",
    "print(data[\"train\"][0][\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 5: Load MedGemma with QLoRA\n",
    "\n",
    "We load MedGemma 4B instruction-tuned model with 4-bit quantization to fit in GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "\n",
    "MODEL_ID = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Determine dtype based on GPU capability\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
    "    compute_dtype = torch.bfloat16\n",
    "    print(\"Using bfloat16 (A100/H100 detected)\")\n",
    "else:\n",
    "    compute_dtype = torch.float16\n",
    "    print(\"Using float16 (T4/V100 detected)\")\n",
    "\n",
    "# QLoRA quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_quant_storage=compute_dtype,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_ID}...\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"\\n‚úì MedGemma loaded!\")\n",
    "print(f\"  Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"  Dtype: {compute_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from typing import Any\n",
    "\n",
    "# ============================================================\n",
    "# LoRA Configuration\n",
    "# ============================================================\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"],\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Custom Data Collator for multimodal inputs\n",
    "# ============================================================\n",
    "def collate_fn(examples: list[dict[str, Any]]):\n",
    "    \"\"\"Process examples with text + images into model input format.\"\"\"\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        images.append([example[\"image\"].convert(\"RGB\")])\n",
    "        texts.append(\n",
    "            processor.apply_chat_template(\n",
    "                example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "            ).strip()\n",
    "        )\n",
    "\n",
    "    # Tokenize texts and process images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Create labels: mask padding and image tokens in loss computation\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # Mask image tokens\n",
    "    image_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map.get(\"boi_token\", \"<image>\")\n",
    "    )\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    if isinstance(image_token_id, int):\n",
    "        labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100  # Additional image placeholder token\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "print(\"‚úì LoRA config and data collator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# ============================================================\n",
    "# Training Configuration\n",
    "# ============================================================\n",
    "OUTPUT_DIR = f\"medgemma-4b-it-{TASK}\"\n",
    "DRIVE_SAVE_DIR = f\"/content/drive/MyDrive/rural_triage_ai/models/{TASK}\"\n",
    "os.makedirs(DRIVE_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Adjust batch size based on GPU memory\n",
    "# A100 (40GB): batch_size=4, grad_accum=4\n",
    "# T4 (16GB):   batch_size=1, grad_accum=16\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
    "    batch_size = 4\n",
    "    grad_accum = 4\n",
    "else:\n",
    "    batch_size = 1\n",
    "    grad_accum = 16\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=(compute_dtype == torch.bfloat16),\n",
    "    fp16=(compute_dtype == torch.float16),\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Create Trainer\n",
    "# ============================================================\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"validation\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Trainer configured!\")\n",
    "print(f\"  Batch size: {batch_size} √ó {grad_accum} grad accum = {batch_size * grad_accum} effective\")\n",
    "print(f\"  Training steps: ~{len(data['train']) // (batch_size * grad_accum)}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ude80 Step 6: Train!\n",
    "\n",
    "This will take approximately:\n",
    "- **A100**: ~1-3 hours (2000 samples, 1 epoch)\n",
    "- **T4**: ~4-8 hours (2000 samples, 1 epoch)\n",
    "\n",
    "You can safely close your laptop ‚Äî training runs in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"   Task: {TASK}\")\n",
    "print(f\"   Model: {MODEL_ID}\")\n",
    "print(f\"   Train samples: {len(data['train'])}\")\n",
    "print(f\"   Val samples: {len(data['validation'])}\")\n",
    "print()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Save Model to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import shutil\n",
    "import json\n",
    "\n",
    "# Save LoRA adapter locally\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Copy to Google Drive for persistence\n",
    "print(f\"Copying model to Google Drive: {DRIVE_SAVE_DIR}\")\n",
    "if os.path.exists(DRIVE_SAVE_DIR):\n",
    "    shutil.rmtree(DRIVE_SAVE_DIR)\n",
    "shutil.copytree(OUTPUT_DIR, DRIVE_SAVE_DIR)\n",
    "\n",
    "# Save training metrics\n",
    "metrics = trainer.state.log_history\n",
    "with open(f\"{DRIVE_SAVE_DIR}/training_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# Save task config for inference\n",
    "task_config = {\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"task\": TASK,\n",
    "    \"class_labels\": CLASS_LABELS,\n",
    "    \"prompt\": PROMPT,\n",
    "    \"lora_dir\": DRIVE_SAVE_DIR,\n",
    "}\n",
    "with open(f\"{DRIVE_SAVE_DIR}/task_config.json\", \"w\") as f:\n",
    "    json.dump(task_config, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Model saved to Google Drive!\")\n",
    "print(f\"  Adapter: {DRIVE_SAVE_DIR}/\")\n",
    "print(f\"  Metrics: {DRIVE_SAVE_DIR}/training_metrics.json\")\n",
    "print(f\"  Config:  {DRIVE_SAVE_DIR}/task_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================\n",
    "# Run inference on validation set\n",
    "# ============================================================\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_pred_text = []\n",
    "num_eval = min(100, len(data[\"validation\"]))  # Evaluate on subset for speed\n",
    "\n",
    "print(f\"Running inference on {num_eval} validation samples...\\n\")\n",
    "\n",
    "for i in range(num_eval):\n",
    "    example = data[\"validation\"][i]\n",
    "    image = example[\"image\"].convert(\"RGB\")\n",
    "    true_label = example[\"label\"]\n",
    "\n",
    "    # Build input (user message only, no assistant response)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": PROMPT},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, tokenize=False\n",
    "    )\n",
    "    model_inputs = processor(\n",
    "        text=inputs, images=[image], return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=compute_dtype)\n",
    "\n",
    "    input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "    \n",
    "    generated = processor.decode(output[0][input_len:], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Parse predicted class\n",
    "    pred_label = -1\n",
    "    for idx, class_name in enumerate(CLASS_LABELS):\n",
    "        if class_name.split(\":\")[0].strip() in generated:\n",
    "            pred_label = idx\n",
    "            break\n",
    "    if pred_label == -1:\n",
    "        pred_label = 0  # Default fallback\n",
    "\n",
    "    y_true.append(true_label)\n",
    "    y_pred.append(pred_label)\n",
    "    y_pred_text.append(generated)\n",
    "\n",
    "    if i < 5:\n",
    "        print(f\"  Sample {i}: True={CLASS_LABELS[true_label]} | Pred={generated}\")\n",
    "\n",
    "# ============================================================\n",
    "# Classification Report\n",
    "# ============================================================\n",
    "short_labels = [c.split(\": \")[1] for c in CLASS_LABELS]\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Classification Report\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_true, y_pred, target_names=short_labels, zero_division=0))\n",
    "\n",
    "# ============================================================\n",
    "# Confusion Matrix\n",
    "# ============================================================\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(CLASS_LABELS))))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "    xticklabels=short_labels, yticklabels=short_labels,\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(f\"Confusion Matrix ‚Äî {TASK.title()} Detection\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{DRIVE_SAVE_DIR}/confusion_matrix.png\", dpi=150)\n",
    "plt.show()\n",
    "print(f\"‚úì Confusion matrix saved to {DRIVE_SAVE_DIR}/confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 9: Single Image Inference Demo\n",
    "\n",
    "Use this cell to test the model on any image. Upload an image or provide a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path: str, task_prompt: str = PROMPT) -> str:\n",
    "    \"\"\"Run inference on a single image and return the prediction.\"\"\"\n",
    "    if image_path.endswith(\".dcm\"):\n",
    "        import pydicom\n",
    "        dcm = pydicom.dcmread(image_path)\n",
    "        arr = dcm.pixel_array.astype(np.float32)\n",
    "        arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-8) * 255\n",
    "        image = Image.fromarray(arr.astype(np.uint8)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": task_prompt},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, tokenize=False\n",
    "    )\n",
    "    model_inputs = processor(\n",
    "        text=inputs, images=[image], return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=compute_dtype)\n",
    "\n",
    "    input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "\n",
    "    response = processor.decode(output[0][input_len:], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Display\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Prediction: {response}\", fontsize=12, pad=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# --- Test on a validation image ---\n",
    "test_example = data[\"validation\"][0]\n",
    "test_path = test_example.get(\"image_path\", None)\n",
    "if test_path and os.path.exists(test_path):\n",
    "    result = predict_image(test_path)\n",
    "else:\n",
    "    # Use the PIL image directly\n",
    "    image = test_example[\"image\"].convert(\"RGB\")\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": PROMPT}]}]\n",
    "    inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    model_inputs = processor(text=inputs, images=[image], return_tensors=\"pt\").to(model.device, dtype=compute_dtype)\n",
    "    input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "    result = processor.decode(output[0][input_len:], skip_special_tokens=True).strip()\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Prediction: {result}\", fontsize=12, pad=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nTrue label: {CLASS_LABELS[test_example['label']]}\")\n",
    "print(f\"Prediction: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "What we did:\n",
    "  ‚úì Loaded MedGemma 4B with QLoRA (4-bit quantization)\n",
    "  ‚úì Prepared {TASK} dataset in conversational format\n",
    "  ‚úì Fine-tuned with SFTTrainer + LoRA adapters\n",
    "  ‚úì Evaluated on validation set with classification metrics\n",
    "  ‚úì Saved LoRA adapter + config to Google Drive\n",
    "\n",
    "Model saved at:\n",
    "  {DRIVE_SAVE_DIR}/\n",
    "\n",
    "To load the fine-tuned model later:\n",
    "  from peft import PeftModel\n",
    "  base_model = AutoModelForImageTextToText.from_pretrained(\"{MODEL_ID}\", ...)\n",
    "  model = PeftModel.from_pretrained(base_model, \"{DRIVE_SAVE_DIR}\")\n",
    "\n",
    "Next steps:\n",
    "  1. Train on full RSNA dataset (100K+ images) for better accuracy\n",
    "  2. Train pneumothorax task (change TASK=\"pneumothorax\" and re-run)\n",
    "  3. Integrate into FastAPI backend (src/api/)\n",
    "  4. Build React Native UI for tablet deployment\n",
    "  5. Submit to MedGemma Impact Challenge!\n",
    "\"\"\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
